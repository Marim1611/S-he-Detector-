{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "from skimage import io,color\n",
    "from skimage.transform import resize\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "import os\n",
    "from  LBP_descriptor import LocalBinaryPatterns\n",
    "import commonfunctions as cf\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import csv\n",
    "from skimage.feature import greycomatrix, greycoprops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get our training data \n",
    "X_train: features of training data.\\\n",
    "Y_train: labels of training data (1-->F, 0--> M).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ICDAR LABELS \n",
    "labels_ICDAR=[]\n",
    "with open(\"our dataset/train_answers.csv\", 'r') as file:\n",
    "    csvreader = csv.reader(file, delimiter=',')\n",
    "    rows= np.array(list(csvreader))[1:].astype(float).astype(int)\n",
    "for row in rows:\n",
    "    labels_ICDAR.append(row[1])\n",
    "    \n",
    "def get_label_ICDAR(img):\n",
    "    if img[0]=='0':\n",
    "        if img[1]=='0': \n",
    "            return labels_ICDAR[int(img[2])-1]\n",
    "        else: \n",
    "            return labels_ICDAR[int(img[1:3])-1]\n",
    "    else: \n",
    "         return labels_ICDAR[int(img[0:3])-1]\n",
    "        \n",
    "\n",
    "\n",
    "def read_labels(path): \n",
    "    y=[]\n",
    "    files = os.listdir(path)\n",
    "\n",
    "    for file in files:\n",
    "        if file[0]=='F':\n",
    "            y.append(0)\n",
    "        elif file[0]=='M':\n",
    "            y.append(1)\n",
    "        else: \n",
    "            y.append(get_label_ICDAR(file[1:4]))\n",
    "          \n",
    "    y=np.array(y).astype(float)\n",
    "    return y \n",
    "\n",
    "Y_train= read_labels(\"Training_data/\")\n",
    "Y_test= read_labels(\"Test_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOG feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HOG(img):\n",
    "    img = np.array(resize(img,(128,64))) \n",
    "    feature_vector, hog_image = hog(img, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(3,3), visualize=True)\n",
    "    return feature_vector,hog_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LBP feature and histogram descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create object from LBP class to use it as our descriptor\n",
    "# takes 2 parameters: number of data (train + test ) and number of neighbors\n",
    "desc = LocalBinaryPatterns(24, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLCM feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GLCM (image):\n",
    "\n",
    "    # convert image to gray\n",
    "    image= cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    featureVector=[]\n",
    "\n",
    "    glcm = greycomatrix(image, distances=[5], angles=[0], levels=256,\n",
    "                            symmetric=True, normed=True)\n",
    "    featureVector.append(greycoprops(glcm, 'dissimilarity')[0, 0])\n",
    "    featureVector.append(greycoprops(glcm, 'correlation')[0, 0])\n",
    "    featureVector.append(greycoprops(glcm, 'contrast')[0, 0])\n",
    "    featureVector.append(greycoprops(glcm, 'homogeneity')[0, 0])\n",
    "    return featureVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data: feature Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  NOTE => to save your time: \n",
    "#  Run this cell once and the features will be saved in external file so that you can read them by running the next cell.\n",
    "X_train=[]\n",
    "HOG_train=[]\n",
    "LBP_train=[]\n",
    "GLCM_train=[]\n",
    "files = os.listdir(\"Training_data/\")\n",
    "i=0\n",
    "for file in files:\n",
    "    # read the image\n",
    "    img = io.imread(\"Training_data/\"+file )\n",
    "\n",
    "    # ------------------- HOG feature------------------------\n",
    "    feature_vector,hog_image=HOG(img)\n",
    "    # --------------------------------------------------------\n",
    "    HOG_train.append(feature_vector)\n",
    "    \n",
    "    #------------------- LBP feature------------------------\n",
    "    img = cf.downSize(img , 0.5)\n",
    "    hist = desc.describe(img)\n",
    "    LBP_train.append(hist)\n",
    "    #--------------------------------------------------------\n",
    "\n",
    "    #------------------- GLCM feature------------------------\n",
    "    img = cv2.imread(\"Training_data/\"+file )  \n",
    "    GLCM_train=GLCM(img)\n",
    "    #--------------------------------------------------------\n",
    "    \n",
    "    # concatenate all the features in X_train   \n",
    "    feature_vector_temp=np.hstack((HOG_train,LBP_train)).tolist()\n",
    "    feature_vector_temp2=np.hstack((feature_vector_temp[0],GLCM_train)).tolist()\n",
    "    X_train.append(feature_vector_temp2)\n",
    "    # reset them for the next img\n",
    "    HOG_train=[]\n",
    "    LBP_train=[]\n",
    "    GLCM_train=[]\n",
    "    \n",
    "#convert to numpy array\n",
    "#X_train=np.array(X_train)\n",
    "\n",
    "#write feature vector of each image in external file\n",
    "with open('training_features.npy', 'wb') as f:\n",
    "    np.save(f, X_train)\n",
    "f.close()  \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read feature vector of train data from the npy file \n",
    "with open('training_features.npy', 'rb') as f:\n",
    "    X_train = np.load(f,allow_pickle=True)\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Test data: feature Extraction\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE => to save your time: \n",
    "# Run this cell once and the features will be saved in external files so that you can read them by running the next cell.\n",
    "X_test=[]\n",
    "HOG_test=[]\n",
    "LBP_test=[]\n",
    "GLCM_test=[]\n",
    "files = os.listdir(\"Test_data/\")\n",
    "\n",
    "for file in files:\n",
    "   \n",
    "    # read te img\n",
    "    img = io.imread(\"Test_data/\" +file )\n",
    "        \n",
    "    #------------------- HOG feature------------------------\n",
    "    feature_vector,hog_image=HOG(img)\n",
    "    HOG_test.append(feature_vector)\n",
    "    #--------------------------------------------------------\n",
    "\n",
    "    # #------------------- LBP feature------------------------\n",
    "    img = cf.downSize(img , 0.5)\n",
    "    hist = desc.describe(img)\n",
    "    LBP_test.append(hist)\n",
    "    #--------------------------------------------------------\n",
    "\n",
    "    #------------------- GLCM feature------------------------\n",
    "    img = cv2.imread(\"Test_data/\"+file )  \n",
    "    GLCM_test=GLCM(img)\n",
    "    #--------------------------------------------------------\n",
    "\n",
    "    # concatenate all the features in X_train\n",
    "    feature_test_temp=(np.hstack((HOG_test,LBP_test))).tolist()\n",
    "    feature_test_temp2=(np.hstack((feature_test_temp[0],GLCM_test))).tolist()\n",
    "    X_test.append( feature_test_temp2 )\n",
    "\n",
    "    # reset them for the next image\n",
    "    HOG_test=[]\n",
    "    LBP_test=[]\n",
    "    GLCM_test=[]\n",
    "\n",
    "    \n",
    "#X_test=np.array(X_test)\n",
    "\n",
    "#write feature vector of test data in external file\n",
    "with open('test_features.npy', 'wb') as f:\n",
    "    np.save(f, X_test)\n",
    "f.close()   \n",
    "#write labels of test data in external file\n",
    "with open('Y_test.npy', 'wb') as f:\n",
    "    np.save(f, Y_test)\n",
    "f.close()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read feature vector of test data from the npy file \n",
    "with open('test_features.npy', 'rb') as f:\n",
    "    X_test = np.load(f ,allow_pickle=True)\n",
    "f.close()  \n",
    "# Read labels of test data from the npy file \n",
    "with open('Y_test.npy', 'rb') as f:\n",
    "    Y_test = np.load(f ,allow_pickle=True)\n",
    "f.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign weights for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206\n"
     ]
    }
   ],
   "source": [
    "# initialize array of ones for initial weights\n",
    "w= np.ones(206)\n",
    "#----------- feature vector partitions -------------------\n",
    "# HOG : 0 => 10800\n",
    "# LBP : 10800 => 11060\n",
    "#---------------------------------------\n",
    "\n",
    "# update weights of each feature\n",
    "w[0: 10800 ] = 2\n",
    "w[ 10800: 11060 ] = 0.5\n",
    "\n",
    "# length of weights should be the same as number of points??!\n",
    "print(len(w))\n",
    "#print(len(X_train[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.51282051282051 %\n"
     ]
    }
   ],
   "source": [
    "clf=RandomForestClassifier(n_estimators=1000)\n",
    "clf.fit(X_train,Y_train)\n",
    "Y_Predicted=clf.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_Predicted)*100,\"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Linear SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 28.205128205128204 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dell\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "clf=LinearSVC(C=100.0, random_state=42)\n",
    "clf.fit(X_train,Y_train )\n",
    "Y_Predicted=clf.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_Predicted)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Adaboost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.82051282051282 %\n"
     ]
    }
   ],
   "source": [
    "clf=AdaBoostClassifier(n_estimators=400)\n",
    "clf.fit(X_train,Y_train)\n",
    "Y_Predicted=clf.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_Predicted)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. KNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.51282051282051 %\n"
     ]
    }
   ],
   "source": [
    "accuracies=[]\n",
    "for k in range(1,50):\n",
    "    clf=KNeighborsClassifier(n_neighbors = k)\n",
    "    clf.fit(X_train,Y_train)\n",
    "    Y_Predicted=clf.predict(X_test)\n",
    "    accuracies.append(metrics.accuracy_score(Y_test, Y_Predicted)*100)\n",
    "\n",
    "print(\"Accuracy:\",accuracies[np.argmax(accuracies)],\"%\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f3a52463084db90f96d29dcfcfd9bf276dba3c521d76c4c38c835392b64a093b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
