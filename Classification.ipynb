{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "from skimage import io,color\n",
    "from skimage.transform import resize\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "import os\n",
    "from  LBP_descriptor import LocalBinaryPatterns\n",
    "import commonfunctions as cf\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get our training data \n",
    "X_train: features of training data.\\\n",
    "Y_train: labels of training data (1-->F, 0--> M).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ICDAR LABELS \n",
    "labels_ICDAR=[]\n",
    "with open(\"our dataset/train_answers.csv\", 'r') as file:\n",
    "    csvreader = csv.reader(file, delimiter=',')\n",
    "    rows= np.array(list(csvreader))[1:].astype(float).astype(int)\n",
    "for row in rows:\n",
    "    labels_ICDAR.append(row[1])\n",
    "    \n",
    "def get_label_ICDAR(img):\n",
    "    if img[0]=='0':\n",
    "        if img[1]=='0': \n",
    "            return labels_ICDAR[int(img[2])-1]\n",
    "        else: \n",
    "            return labels_ICDAR[int(img[1:3])-1]\n",
    "    else: \n",
    "         return labels_ICDAR[int(img[0:3])-1]\n",
    "        \n",
    "\n",
    "\n",
    "def read_labels(path): \n",
    "    y=[]\n",
    "    files = os.listdir(path)\n",
    "\n",
    "    for file in files:\n",
    "        if file[0]=='F':\n",
    "            y.append(0)\n",
    "        elif file[0]=='M':\n",
    "            y.append(1)\n",
    "        else: \n",
    "            y.append(get_label_ICDAR(file[1:4]))\n",
    "          \n",
    "    y=np.array(y).astype(float)\n",
    "    return y \n",
    "\n",
    "Y_train= read_labels(\"Training_data/\")\n",
    "Y_test= read_labels(\"Test_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOG feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HOG(img):\n",
    "    img = np.array(resize(img,(128,64))) \n",
    "    feature_vector, hog_image = hog(img, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(3,3), visualize=True)\n",
    "    return feature_vector,hog_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LBP feature and histogram descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create object from LBP class to use it as our descriptor\n",
    "# takes 2 parameters: number of data (train + test ) and number of neighbors\n",
    "desc = LocalBinaryPatterns(24, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  NOTE => to save your time: \n",
    "#  Run this cell once and the features will be saved in external file so that you can read them by running the next cell.\n",
    "X_train=[]\n",
    "HOG_train=[]\n",
    "LBP_train=[]\n",
    "files = os.listdir(\"Training_data/\")\n",
    "i=0\n",
    "for file in files:\n",
    "    # read the image\n",
    "    img = io.imread(\"Training_data/\"+file )\n",
    "\n",
    "    # ------------------- HOG feature------------------------\n",
    "    feature_vector,hog_image=HOG(img)\n",
    "    # --------------------------------------------------------\n",
    "    HOG_train.append(feature_vector)\n",
    "    \n",
    "    #------------------- LBP feature------------------------\n",
    "    \n",
    "    img = cf.downSize(img , 0.5)\n",
    "    hist = desc.describe(img)\n",
    "    LBP_train.append(hist)\n",
    "    #--------------------------------------------------------\n",
    "    \n",
    "    # concatenate all the features in X_train   \n",
    "    feature_vector_temp=np.hstack((HOG_train,LBP_train)).tolist()\n",
    "    X_train.append(feature_vector_temp[0])\n",
    "    \n",
    "    # reset them for the next img\n",
    "    HOG_train=[]\n",
    "    LBP_train=[]\n",
    "    \n",
    "#convert to numpy array\n",
    "#X_train=np.array(X_train)\n",
    "\n",
    "#write feature vector of each image in external file\n",
    "with open('training_features.npy', 'wb') as f:\n",
    "    np.save(f, X_train)\n",
    "f.close()  \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238\n"
     ]
    }
   ],
   "source": [
    "print(len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read feature vector of train data from the npy file \n",
    "with open('training_features.npy', 'rb') as f:\n",
    "    X_train = np.load(f,allow_pickle=True)\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get our test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE => to save your time: \n",
    "# Run this cell once and the features will be saved in external files so that you can read them by running the next cell.\n",
    "X_test=[]\n",
    "HOG_test=[]\n",
    "LBP_test=[]\n",
    "\n",
    "files = os.listdir(\"Test_data/\")\n",
    "\n",
    "for file in files:\n",
    "   \n",
    "    # read te img\n",
    "    img = io.imread(\"Test_data/\" +file )\n",
    "        \n",
    "    #------------------- HOG feature------------------------\n",
    "    feature_vector,hog_image=HOG(img)\n",
    "    HOG_test.append(feature_vector)\n",
    "   \n",
    "    \n",
    "    # #------------------- LBP feature------------------------\n",
    "    img = cf.downSize(img , 0.5)\n",
    "    hist = desc.describe(img)\n",
    "    LBP_test.append(hist)\n",
    "    #--------------------------------------------------------\n",
    "\n",
    "    # concatenate all the features in X_train\n",
    "    feature_test_temp=(np.hstack((HOG_test,LBP_test))).tolist()\n",
    "    X_test.append( feature_test_temp[0] )\n",
    "\n",
    "    # reset them for the next image\n",
    "    HOG_test=[]\n",
    "    LBP_test=[]\n",
    "\n",
    "    \n",
    "#X_test=np.array(X_test)\n",
    "\n",
    "#write feature vector of test data in external file\n",
    "with open('test_features.npy', 'wb') as f:\n",
    "    np.save(f, X_test)\n",
    "f.close()   \n",
    "#write labels of test data in external file\n",
    "with open('Y_test.npy', 'wb') as f:\n",
    "    np.save(f, Y_test)\n",
    "f.close()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read feature vector of test data from the npy file \n",
    "with open('test_features.npy', 'rb') as f:\n",
    "    X_test = np.load(f ,allow_pickle=True)\n",
    "f.close()  \n",
    "# Read labels of test data from the npy file \n",
    "with open('Y_test.npy', 'rb') as f:\n",
    "    Y_test = np.load(f ,allow_pickle=True)\n",
    "f.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign weights for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206\n"
     ]
    }
   ],
   "source": [
    "# initialize array of ones for initial weights\n",
    "w= np.ones(206)\n",
    "#----------- feature vector partitions -------------------\n",
    "# HOG : 0 => 10800\n",
    "# LBP : 10800 => 11060\n",
    "#---------------------------------------\n",
    "\n",
    "# update weights of each feature\n",
    "w[0: 10800 ] = 2\n",
    "w[ 10800: 11060 ] = 0.5\n",
    "\n",
    "# length of weights should be the same as number of points??!\n",
    "print(len(w))\n",
    "#print(len(X_train[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238 238\n",
      "Accuracy: 76.89075630252101 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf=RandomForestClassifier(n_estimators=1000)\n",
    "clf.fit(X_train,Y_train)\n",
    "Y_Predicted=clf.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_Predicted)*100,\"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Linear SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 78.57142857142857 %\n",
      "6830\n"
     ]
    }
   ],
   "source": [
    "clf=LinearSVC(C=50.0, random_state=42)\n",
    "clf.fit(X_train,Y_train )\n",
    "Y_Predicted=clf.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_Predicted)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Adaboost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.83193277310924 %\n"
     ]
    }
   ],
   "source": [
    "clf=AdaBoostClassifier(n_estimators=500)\n",
    "clf.fit(X_train,Y_train)\n",
    "Y_Predicted=clf.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_Predicted)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. KNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.05042016806722 %\n"
     ]
    }
   ],
   "source": [
    "accuracies=[]\n",
    "for k in range(1,30):\n",
    "    clf=KNeighborsClassifier(n_neighbors = k)\n",
    "    clf.fit(X_train,Y_train)\n",
    "    Y_Predicted=clf.predict(X_test)\n",
    "    accuracies.append(metrics.accuracy_score(Y_test, Y_Predicted)*100)\n",
    "\n",
    "print(\"Accuracy:\",accuracies[np.argmax(accuracies)],\"%\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a2ab3825ac7005fb7b26f112e9c99ae62f464c629e30b0d534c3b931b6cbc3ff"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
